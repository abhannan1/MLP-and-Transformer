{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "5XG5jB0pHW2i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "num_heads =4"
      ],
      "metadata": {
        "id": "wBlp43VNoDgF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sRPWh0EKsYLG",
        "outputId": "acf84c96-e669-4e27-d250-3ae56e1f3b77"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "650KpVW8o0dz",
        "outputId": "6b47e079-9ed7-44c4-9e2e-baeb4dea068d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7eb46c221b10>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "JjmRhELIpeOQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DuAx-jLOp6pk",
        "outputId": "797d5301-e792-4b8f-cb08-663f93cb536d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"/content/drive/MyDrive/ng-video-lecture-master\")"
      ],
      "metadata": {
        "id": "yFUDivXjp941"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', \"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "rlOlf1JYo2Uy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))"
      ],
      "metadata": {
        "id": "7-8HrLhGqEQ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(chars)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNqYu6uVqeE7",
        "outputId": "5798b1fb-ec21-4ec7-c546-dadb200e337b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}"
      ],
      "metadata": {
        "id": "YnoIpPJLqtGW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s : [stoi.get(c) for c in s ]\n",
        "decode = lambda l : \"\".join(itos.get(i) for i in l)"
      ],
      "metadata": {
        "id": "L68HVK_trYU_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)"
      ],
      "metadata": {
        "id": "uMZt9YLTsA5y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int (0.9*len(data))"
      ],
      "metadata": {
        "id": "mDP4LgORsDCF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "bX3oPz11sG9a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[:block_size]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xByymNC-aPvL",
        "outputId": "d67ada9a-ff98-4f9d-d94c-fc35f72e8b5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[1:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR9KEs5KaSIs",
        "outputId": "2597e911-eeb1-4acb-de62-559ede5e52da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, 53,\n",
              "        56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for ch in range(block_size):\n",
        "  context =  x[:ch+1]\n",
        "  target = y[ch]\n",
        "  print(f\"context : {context}, target:{target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG_c6DAQkhXU",
        "outputId": "b41934d2-3778-4b01-ebf2-7f6fcad96f95"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context : tensor([18]), target:47\n",
            "context : tensor([18, 47]), target:56\n",
            "context : tensor([18, 47, 56]), target:57\n",
            "context : tensor([18, 47, 56, 57]), target:58\n",
            "context : tensor([18, 47, 56, 57, 58]), target:1\n",
            "context : tensor([18, 47, 56, 57, 58,  1]), target:15\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15]), target:47\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47]), target:58\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), target:47\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]), target:64\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]), target:52\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]), target:10\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]), target:0\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]), target:14\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]), target:44\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]), target:53\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53]), target:56\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43]), target:1\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1]), target:61\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43]), target:1\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1]), target:54\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54]), target:56\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56]), target:53\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]), target:41\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]), target:43\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]), target:42\n",
            "context : tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]), target:1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(split):\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size,))\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y = x.to(device), y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "uIASHmqklESk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      xb,yb = get_batches(split)\n",
        "      logits, loss = m(xb, yb)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "avpvaoPZ8Cx7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b,t = xb.shape\n",
        "xb.view(b*t).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "4o7oSIuR46_f",
        "outputId": "e72e6552-55dc-451e-f7c7-77ee1df28e98"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-308abed74937>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "iniTbhTnpmpB",
        "outputId": "7144f553-d129-4f5a-9d60-69c87396333d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'xb' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6f30c979ba7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'xb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None ):\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for k in range(max_new_tokens):\n",
        "      logits, loss = self(idx)\n",
        "      if k == 0 :\n",
        "        print(logits.shape)\n",
        "      logits= logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "4KH37dNWp0Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long , device=device)\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP_RamOT1X8W",
        "outputId": "46410674-042d-467f-95bb-ef2bea0b3994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 65])\n",
            "tensor(4.6057, grad_fn=<NllLossBackward0>)\n",
            "4.605657577514648\n",
            "torch.Size([1, 1, 65])\n",
            "\n",
            "DtQOLwf'RIPD'Hmhs xvLwxat:sbYZwLEHy'pryOqOZm$!s$zheQ,;eMkjm,CQufzLw!iWn.e!zDGxA PsS3zdKY!KzGCeJydKJS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((2,2), dtype=torch.long , device=device)\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[1].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrEH8IxNt6M_",
        "outputId": "62be7653-e6e1-4c46-d8c6-7ad48bc3f5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 65])\n",
            "\n",
            "\n",
            "rFFJSak!g!wWSa.WxbjBiDeQ:scqpqvWrxb-du$WDaJWuBr-g-sOZKYwPifPV&& PDzluBatz$LDt:aRyU:aZfBUvfp:ZQsbZhli\n",
            "torch.Size([2, 2, 65])\n",
            "\n",
            "\n",
            "C;tBpMVAMUuAzduxvuh3DtB\n",
            "CrKqoiGBx;nVTAkdxJKZmXi-$A.c!Z3zhMx-EsVLYG,X mWQI\n",
            "kiwBv fnIaYyVV;apjoLEm-fRx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "tM_OXtd67ra0",
        "outputId": "590310b0-8c98-4ea8-ae56-cd6b2fc636a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-09b232d79314>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm"
      ],
      "metadata": {
        "id": "idJ7lPowFK4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      xb,yb = get_batches(split)\n",
        "      logits, loss = m(xb, yb)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "9q51e2vME_OD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batches(\"train\")\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAPmUS0KD4iU",
        "outputId": "862116ad-6904-4131-a997-b885bd4232dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.6097, val loss 4.6070\n",
            "step 100: train loss 4.4848, val loss 4.4847\n",
            "step 200: train loss 4.3690, val loss 4.3658\n",
            "step 300: train loss 4.2497, val loss 4.2539\n",
            "step 400: train loss 4.1448, val loss 4.1477\n",
            "step 500: train loss 4.0380, val loss 4.0378\n",
            "step 600: train loss 3.9405, val loss 3.9473\n",
            "step 700: train loss 3.8495, val loss 3.8498\n",
            "step 800: train loss 3.7536, val loss 3.7556\n",
            "step 900: train loss 3.6671, val loss 3.6765\n",
            "step 1000: train loss 3.5890, val loss 3.5951\n",
            "step 1100: train loss 3.5185, val loss 3.5208\n",
            "step 1200: train loss 3.4463, val loss 3.4531\n",
            "step 1300: train loss 3.3796, val loss 3.3832\n",
            "step 1400: train loss 3.3113, val loss 3.3224\n",
            "step 1500: train loss 3.2539, val loss 3.2572\n",
            "step 1600: train loss 3.2028, val loss 3.2102\n",
            "step 1700: train loss 3.1493, val loss 3.1562\n",
            "step 1800: train loss 3.1044, val loss 3.1154\n",
            "step 1900: train loss 3.0563, val loss 3.0636\n",
            "step 2000: train loss 3.0186, val loss 3.0271\n",
            "step 2100: train loss 2.9771, val loss 2.9832\n",
            "step 2200: train loss 2.9388, val loss 2.9529\n",
            "step 2300: train loss 2.9092, val loss 2.9166\n",
            "step 2400: train loss 2.8763, val loss 2.8845\n",
            "step 2500: train loss 2.8485, val loss 2.8600\n",
            "step 2600: train loss 2.8155, val loss 2.8308\n",
            "step 2700: train loss 2.7971, val loss 2.8044\n",
            "step 2800: train loss 2.7731, val loss 2.7800\n",
            "step 2900: train loss 2.7508, val loss 2.7595\n",
            "step 3000: train loss 2.7291, val loss 2.7360\n",
            "step 3100: train loss 2.7064, val loss 2.7198\n",
            "step 3200: train loss 2.6949, val loss 2.7021\n",
            "step 3300: train loss 2.6794, val loss 2.6895\n",
            "step 3400: train loss 2.6630, val loss 2.6749\n",
            "step 3500: train loss 2.6536, val loss 2.6637\n",
            "step 3600: train loss 2.6414, val loss 2.6436\n",
            "step 3700: train loss 2.6244, val loss 2.6356\n",
            "step 3800: train loss 2.6103, val loss 2.6258\n",
            "step 3900: train loss 2.6005, val loss 2.6213\n",
            "step 4000: train loss 2.5917, val loss 2.6015\n",
            "step 4100: train loss 2.5817, val loss 2.5988\n",
            "step 4200: train loss 2.5785, val loss 2.5882\n",
            "step 4300: train loss 2.5663, val loss 2.5834\n",
            "step 4400: train loss 2.5663, val loss 2.5753\n",
            "step 4500: train loss 2.5627, val loss 2.5719\n",
            "step 4600: train loss 2.5501, val loss 2.5645\n",
            "step 4700: train loss 2.5448, val loss 2.5598\n",
            "step 4800: train loss 2.5359, val loss 2.5526\n",
            "step 4900: train loss 2.5326, val loss 2.5432\n",
            "2.548557758331299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXEaGnV_E11m",
        "outputId": "5b9bc2cf-dc51-4f2d-ffa3-7b6ba2696dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2, 65])\n",
            "\n",
            "\n",
            "Bus beis, w.\n",
            "Fonirchenomastone heavare I ope t. kirugh?\n",
            "Yvebreforiveenge nesed fes  PSLAEcuan3a? LEn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "head_size = 16\n",
        "\n",
        "query = nn.Linear(C,head_size, bias= False)\n",
        "key = nn.Linear(C,head_size, bias= False)\n",
        "value = nn.Linear(C,head_size, bias= False)\n",
        "\n",
        "q = query(x) #(B,T,head_size)\n",
        "k = key(x) #(B,T,head_size)\n",
        "v = value(x) #(B,T,head_size)\n",
        "\n",
        "wei = q @ k.transpose(-2,-1) # (B,head_size,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(wei[0])\n",
        "print(wei.shape)\n",
        "\n",
        "out = wei @ v\n",
        "\n",
        "print(out[0])\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD1XFiBYomMV",
        "outputId": "34d81f75-ae2e-42dc-96f0-5a42e3c3f234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.6119, 0.3881, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0711, 0.1138, 0.8150, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3576, 0.0473, 0.1256, 0.4695, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0063, 0.2960, 0.0177, 0.1327, 0.5473, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0365, 0.1685, 0.3274, 0.1337, 0.0643, 0.2696, 0.0000, 0.0000],\n",
            "        [0.1839, 0.0514, 0.1499, 0.3818, 0.0289, 0.1437, 0.0603, 0.0000],\n",
            "        [0.0533, 0.1844, 0.0814, 0.0034, 0.0140, 0.0214, 0.5796, 0.0625]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "torch.Size([4, 8, 8])\n",
            "tensor([[-2.0433e-01,  2.5407e-01,  5.9613e-01,  1.2759e+00, -2.8305e-01,\n",
            "          7.6875e-01,  1.5049e-02, -6.1012e-01,  1.0102e+00, -4.7575e-01,\n",
            "         -7.0992e-01, -2.3299e-01, -1.6077e+00,  9.3972e-01,  4.6763e-02,\n",
            "         -1.2855e-02],\n",
            "        [-2.0186e-01,  5.4153e-02,  1.5977e-02,  9.8759e-01, -2.9155e-01,\n",
            "          4.3639e-01,  3.3963e-01, -2.4520e-01,  9.5238e-01, -3.4324e-01,\n",
            "         -1.4182e-01,  1.6721e-01, -1.1580e+00,  5.9711e-01, -2.2433e-01,\n",
            "         -5.0586e-02],\n",
            "        [-7.4773e-01, -7.4945e-01, -8.7280e-01,  5.7553e-01, -7.7418e-01,\n",
            "          2.2384e-01,  6.2974e-01,  5.6421e-01,  3.4452e-01, -3.5345e-01,\n",
            "         -8.3407e-03,  2.9858e-02,  2.3019e-01,  3.1014e-01, -4.3315e-01,\n",
            "         -1.2467e-01],\n",
            "        [-4.1769e-01,  1.4628e-01,  3.5286e-01,  7.8950e-01, -2.6560e-01,\n",
            "          3.6221e-01,  1.6573e-01, -1.6002e-01,  2.5793e-01, -6.6355e-01,\n",
            "          3.0673e-02,  6.3296e-02, -8.3764e-01,  4.3088e-01, -5.3381e-01,\n",
            "         -1.3689e-01],\n",
            "        [-1.8143e-01,  2.4738e-01, -4.0686e-01,  3.3855e-01,  4.6025e-01,\n",
            "         -5.5152e-01,  1.6849e-02, -5.3291e-02, -4.4137e-01, -6.4385e-01,\n",
            "          5.8975e-01,  6.1067e-01, -9.0791e-02, -5.8094e-01, -8.7925e-01,\n",
            "          2.7546e-01],\n",
            "        [-2.3515e-01, -3.0238e-01, -3.7144e-01,  4.4667e-01, -2.7888e-01,\n",
            "         -7.8027e-02,  2.2741e-01,  3.3346e-01, -6.3465e-02, -3.6437e-01,\n",
            "          2.1765e-01,  2.2064e-01,  1.8757e-01,  1.3530e-01, -3.4066e-01,\n",
            "          3.6233e-02],\n",
            "        [-3.0787e-01,  1.5869e-03,  1.5577e-01,  5.1863e-01, -2.0667e-01,\n",
            "          1.6965e-01,  8.7295e-02,  2.7874e-02,  9.8489e-03, -5.3900e-01,\n",
            "          8.4739e-02,  8.5495e-02, -3.5628e-01,  2.9867e-01, -4.2852e-01,\n",
            "         -6.0251e-02],\n",
            "        [-3.8511e-01, -4.1045e-01, -3.5167e-01, -2.1425e-01, -2.9344e-01,\n",
            "          3.7854e-01,  8.5249e-02,  1.5849e-01,  4.0317e-01,  5.3896e-02,\n",
            "         -3.6858e-01, -2.0707e-01, -4.9371e-02,  4.7529e-01, -5.7979e-02,\n",
            "         -5.6766e-02]], grad_fn=<SelectBackward0>)\n",
            "torch.Size([4, 8, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39CwUYrdITz4",
        "outputId": "bebe17cc-b1f8-45c8-dc48-a756768f8346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # print(k)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # print(wei.shape)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "\n",
        "    out = wei @ v\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "K7Wu4smA9R0E"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_head = Head(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None ):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # print(\"targets \\n\",targets)\n",
        "      B,T,C = logits.shape\n",
        "      # print(\"logits \\n\",logits.shape)\n",
        "      logits = logits.view(B*T,C)\n",
        "      # print(\"logits reshape \\n\", logits.shape)\n",
        "      targets = targets.view(B*T)\n",
        "      # print(\"targets reshape \\n\", targets.shape)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      # print(\"loss \\n\",loss)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for k in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      # if k == 0 :\n",
        "      #   print(logits.shape)\n",
        "      logits= logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "JPCfYf-gVHNa"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long , device=device)\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "# print(loss.item())\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 2)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wy6qVL652S8",
        "outputId": "33095a11-a6b6-46c7-d4ae-12d2256897aa"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "OI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "vQq6N5CzBkAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batches(\"train\")\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caMwiiTg6LQk",
        "outputId": "b358e69d-bb34-4ec6-a9c8-db367edc333f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1604, val loss 4.1592\n",
            "step 100: train loss 3.0700, val loss 3.0790\n",
            "step 200: train loss 2.8853, val loss 2.8858\n",
            "step 300: train loss 2.7945, val loss 2.8054\n",
            "step 400: train loss 2.7451, val loss 2.7495\n",
            "step 500: train loss 2.7218, val loss 2.7288\n",
            "step 600: train loss 2.6936, val loss 2.6990\n",
            "step 700: train loss 2.6826, val loss 2.6786\n",
            "step 800: train loss 2.6523, val loss 2.6694\n",
            "step 900: train loss 2.6418, val loss 2.6481\n",
            "step 1000: train loss 2.6226, val loss 2.6292\n",
            "step 1100: train loss 2.5974, val loss 2.6120\n",
            "step 1200: train loss 2.5796, val loss 2.5781\n",
            "step 1300: train loss 2.5577, val loss 2.5677\n",
            "step 1400: train loss 2.5265, val loss 2.5427\n",
            "step 1500: train loss 2.4906, val loss 2.5050\n",
            "step 1600: train loss 2.4654, val loss 2.4656\n",
            "step 1700: train loss 2.4360, val loss 2.4491\n",
            "step 1800: train loss 2.4255, val loss 2.4362\n",
            "step 1900: train loss 2.4141, val loss 2.4338\n",
            "step 2000: train loss 2.4000, val loss 2.4176\n",
            "step 2100: train loss 2.3903, val loss 2.4095\n",
            "step 2200: train loss 2.3922, val loss 2.4120\n",
            "step 2300: train loss 2.3834, val loss 2.3957\n",
            "step 2400: train loss 2.3769, val loss 2.3993\n",
            "step 2500: train loss 2.3729, val loss 2.3898\n",
            "step 2600: train loss 2.3667, val loss 2.3933\n",
            "step 2700: train loss 2.3678, val loss 2.3867\n",
            "step 2800: train loss 2.3715, val loss 2.3773\n",
            "step 2900: train loss 2.3611, val loss 2.3743\n",
            "step 3000: train loss 2.3486, val loss 2.3718\n",
            "step 3100: train loss 2.3503, val loss 2.3808\n",
            "step 3200: train loss 2.3482, val loss 2.3694\n",
            "step 3300: train loss 2.3556, val loss 2.3709\n",
            "step 3400: train loss 2.3405, val loss 2.3691\n",
            "step 3500: train loss 2.3391, val loss 2.3592\n",
            "step 3600: train loss 2.3482, val loss 2.3620\n",
            "step 3700: train loss 2.3328, val loss 2.3543\n",
            "step 3800: train loss 2.3354, val loss 2.3536\n",
            "step 3900: train loss 2.3340, val loss 2.3596\n",
            "step 4000: train loss 2.3345, val loss 2.3498\n",
            "step 4100: train loss 2.3331, val loss 2.3664\n",
            "step 4200: train loss 2.3237, val loss 2.3541\n",
            "step 4300: train loss 2.3285, val loss 2.3525\n",
            "step 4400: train loss 2.3249, val loss 2.3545\n",
            "step 4500: train loss 2.3319, val loss 2.3588\n",
            "step 4600: train loss 2.3273, val loss 2.3471\n",
            "step 4700: train loss 2.3200, val loss 2.3466\n",
            "step 4800: train loss 2.3150, val loss 2.3445\n",
            "step 4900: train loss 2.3132, val loss 2.3602\n",
            "2.446246862411499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((2,2), dtype=torch.long , device=device)\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[1].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZVq1f5N6M-5",
        "outputId": "4ab9182c-a79c-4210-94dd-3d4baf4b2f8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Wewealif-\n",
            "Wamyouoruss at-\n",
            "chat yo wre gsakee. Sharde; me.\n",
            "CETHO:\n",
            "Why omallteaterel wo yary meepes fi\n",
            "\n",
            "\n",
            "'tlo, nsu idotof st-th ther,\n",
            "Thu is ond tuth yo led lothe witlal arel wiake acing thy to.\n",
            "\n",
            "Frolo.\n",
            "\n",
            "L\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "maTZAFac6cb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # print(q)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # print(wei)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward (self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "5yl3Alr9JNwu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiheadAttention(num_heads=num_heads,head_size=int(n_embd/num_heads) )\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None ):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # print(\"targets \\n\",targets)\n",
        "      B,T,C = logits.shape\n",
        "      # print(\"logits \\n\",logits.shape)\n",
        "      logits = logits.view(B*T,C)\n",
        "      # print(\"logits reshape \\n\", logits.shape)\n",
        "      targets = targets.view(B*T)\n",
        "      # print(\"targets reshape \\n\", targets.shape)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      # print(\"loss \\n\",loss)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for k in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      # if k == 0 :\n",
        "      #   print(logits.shape)\n",
        "      logits= logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "YGppvsR-4O1B"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long , device=device)\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1jxbXw07wlJ",
        "outputId": "6dfac2a1-6769-48ef-87ff-2164ed43bfbe"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 65])\n",
            "tensor(4.2078, grad_fn=<NllLossBackward0>)\n",
            "4.207807540893555\n",
            "\n",
            "a3E$fSsISf:lTPZbpOlP-uap.doVgifTenCiyKuJLWHHbwd:$duddGHHoE.xdfuOWdBAzcMjttbeZNc$Nos,bvWX,d$jd-!pB:Y'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "0dRNEpJ1Bmt5"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batches(\"train\")\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJmM2meb6ueC",
        "outputId": "1e61a61d-42b4-4349-fed4-ddd33e2e30e0"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1996, val loss 4.1988\n",
            "step 100: train loss 2.9423, val loss 2.9562\n",
            "step 200: train loss 2.7445, val loss 2.7376\n",
            "step 300: train loss 2.6704, val loss 2.6678\n",
            "step 400: train loss 2.6191, val loss 2.6153\n",
            "step 500: train loss 2.5927, val loss 2.5805\n",
            "step 600: train loss 2.5545, val loss 2.5423\n",
            "step 700: train loss 2.5246, val loss 2.5237\n",
            "step 800: train loss 2.4962, val loss 2.4903\n",
            "step 900: train loss 2.4708, val loss 2.4576\n",
            "step 1000: train loss 2.4401, val loss 2.4358\n",
            "step 1100: train loss 2.4153, val loss 2.4145\n",
            "step 1200: train loss 2.3930, val loss 2.3988\n",
            "step 1300: train loss 2.3862, val loss 2.3904\n",
            "step 1400: train loss 2.3503, val loss 2.3607\n",
            "step 1500: train loss 2.3376, val loss 2.3537\n",
            "step 1600: train loss 2.3316, val loss 2.3485\n",
            "step 1700: train loss 2.3140, val loss 2.3316\n",
            "step 1800: train loss 2.2997, val loss 2.3139\n",
            "step 1900: train loss 2.2836, val loss 2.3049\n",
            "step 2000: train loss 2.2748, val loss 2.2927\n",
            "step 2100: train loss 2.2741, val loss 2.2914\n",
            "step 2200: train loss 2.2493, val loss 2.2785\n",
            "step 2300: train loss 2.2423, val loss 2.2678\n",
            "step 2400: train loss 2.2407, val loss 2.2619\n",
            "step 2500: train loss 2.2280, val loss 2.2588\n",
            "step 2600: train loss 2.2224, val loss 2.2471\n",
            "step 2700: train loss 2.2187, val loss 2.2435\n",
            "step 2800: train loss 2.1920, val loss 2.2322\n",
            "step 2900: train loss 2.1945, val loss 2.2245\n",
            "step 3000: train loss 2.1942, val loss 2.2331\n",
            "step 3100: train loss 2.1878, val loss 2.2154\n",
            "step 3200: train loss 2.1772, val loss 2.2237\n",
            "step 3300: train loss 2.1806, val loss 2.2146\n",
            "step 3400: train loss 2.1721, val loss 2.2085\n",
            "step 3500: train loss 2.1636, val loss 2.2035\n",
            "step 3600: train loss 2.1597, val loss 2.1991\n",
            "step 3700: train loss 2.1533, val loss 2.2069\n",
            "step 3800: train loss 2.1404, val loss 2.1998\n",
            "step 3900: train loss 2.1477, val loss 2.1879\n",
            "step 4000: train loss 2.1415, val loss 2.1930\n",
            "step 4100: train loss 2.1299, val loss 2.1799\n",
            "step 4200: train loss 2.1303, val loss 2.1930\n",
            "step 4300: train loss 2.1284, val loss 2.2008\n",
            "step 4400: train loss 2.1251, val loss 2.1757\n",
            "step 4500: train loss 2.1184, val loss 2.1768\n",
            "step 4600: train loss 2.1045, val loss 2.1816\n",
            "step 4700: train loss 2.1144, val loss 2.1804\n",
            "step 4800: train loss 2.1142, val loss 2.1718\n",
            "step 4900: train loss 2.1005, val loss 2.1722\n",
            "2.1326568126678467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "id": "7OxLOe786xyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c67b6f54-4a47-4d69-dbaa-f43fcf0056df"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "her shave ker; are kes brik pon,\n",
            "Dwitetl nga dith me gred mobeck.\n",
            "\n",
            "QUEENE:\n",
            "Wi's pre, and that feer'w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # print(q)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # print(wei)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward (self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.sa_heads = MultiheadAttention(num_heads=num_heads,head_size=int(n_embd/num_heads) )\n",
        "    self.feedforward = FeedForward(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None ):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.feedforward(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # print(\"targets \\n\",targets)\n",
        "      B,T,C = logits.shape\n",
        "      # print(\"logits \\n\",logits.shape)\n",
        "      logits = logits.view(B*T,C)\n",
        "      # print(\"logits reshape \\n\", logits.shape)\n",
        "      targets = targets.view(B*T)\n",
        "      # print(\"targets reshape \\n\", targets.shape)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      # print(\"loss \\n\",loss)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for k in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      # if k == 0 :\n",
        "      #   print(logits.shape)\n",
        "      logits= logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "w7ikNWihivHp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long , device=device)\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(loss.item())\n",
        "\n",
        "\n",
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "  if iter % eval_interval == 0:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batches(\"train\")\n",
        "  logits, loss = m(xb, yb)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBfGMtot7Si0",
        "outputId": "d44e3c60-4950-4eec-a792-55ba1a295a5b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([512, 65])\n",
            "tensor(4.1557, grad_fn=<NllLossBackward0>)\n",
            "4.155739784240723\n",
            "\n",
            "La zM'VSGG q-lXmmqhesXK$Lyi&PGHiWQFkPnQe;NrUWCIYDcRlKhEVHmIH3&xGAKVQfSFe$SJTOnNiH J,\n",
            "CoiEr\n",
            "yfzH3XKik\n",
            "step 0: train loss 4.1576, val loss 4.1558\n",
            "step 100: train loss 2.9472, val loss 2.9678\n",
            "step 200: train loss 2.6895, val loss 2.6957\n",
            "step 300: train loss 2.6149, val loss 2.6165\n",
            "step 400: train loss 2.5688, val loss 2.5715\n",
            "step 500: train loss 2.5335, val loss 2.5340\n",
            "step 600: train loss 2.4837, val loss 2.4782\n",
            "step 700: train loss 2.4472, val loss 2.4437\n",
            "step 800: train loss 2.4262, val loss 2.4155\n",
            "step 900: train loss 2.4078, val loss 2.3903\n",
            "step 1000: train loss 2.3779, val loss 2.3673\n",
            "step 1100: train loss 2.3549, val loss 2.3620\n",
            "step 1200: train loss 2.3337, val loss 2.3387\n",
            "step 1300: train loss 2.3113, val loss 2.3074\n",
            "step 1400: train loss 2.2981, val loss 2.2924\n",
            "step 1500: train loss 2.2780, val loss 2.2920\n",
            "step 1600: train loss 2.2634, val loss 2.2750\n",
            "step 1700: train loss 2.2535, val loss 2.2613\n",
            "step 1800: train loss 2.2412, val loss 2.2596\n",
            "step 1900: train loss 2.2279, val loss 2.2304\n",
            "step 2000: train loss 2.2193, val loss 2.2319\n",
            "step 2100: train loss 2.1987, val loss 2.2092\n",
            "step 2200: train loss 2.1770, val loss 2.2125\n",
            "step 2300: train loss 2.1786, val loss 2.2148\n",
            "step 2400: train loss 2.1691, val loss 2.1958\n",
            "step 2500: train loss 2.1571, val loss 2.2021\n",
            "step 2600: train loss 2.1463, val loss 2.1829\n",
            "step 2700: train loss 2.1409, val loss 2.1841\n",
            "step 2800: train loss 2.1363, val loss 2.1745\n",
            "step 2900: train loss 2.1300, val loss 2.1747\n",
            "step 3000: train loss 2.1191, val loss 2.1563\n",
            "step 3100: train loss 2.1192, val loss 2.1608\n",
            "step 3200: train loss 2.1065, val loss 2.1633\n",
            "step 3300: train loss 2.1037, val loss 2.1531\n",
            "step 3400: train loss 2.0972, val loss 2.1499\n",
            "step 3500: train loss 2.0994, val loss 2.1510\n",
            "step 3600: train loss 2.0889, val loss 2.1468\n",
            "step 3700: train loss 2.0848, val loss 2.1383\n",
            "step 3800: train loss 2.0721, val loss 2.1492\n",
            "step 3900: train loss 2.0625, val loss 2.1338\n",
            "step 4000: train loss 2.0684, val loss 2.1375\n",
            "step 4100: train loss 2.0632, val loss 2.1331\n",
            "step 4200: train loss 2.0544, val loss 2.1319\n",
            "step 4300: train loss 2.0503, val loss 2.1285\n",
            "step 4400: train loss 2.0493, val loss 2.1333\n",
            "step 4500: train loss 2.0441, val loss 2.1282\n",
            "step 4600: train loss 2.0410, val loss 2.1111\n",
            "step 4700: train loss 2.0400, val loss 2.1040\n",
            "step 4800: train loss 2.0348, val loss 2.1096\n",
            "step 4900: train loss 2.0239, val loss 2.1009\n",
            "2.0998966693878174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx=idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djXEUQqV7hoK",
        "outputId": "4c06e3ef-052b-4577-c7b2-3f917d5eb70e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And the sim he balown as lispakeorden piceed of\n",
            "pengaing sight have,\n",
            "Wike lord fajell if't uss me pa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    # print(q)\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5\n",
        "    # print(wei)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward (self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout)\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,n_embd,n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd//num_heads\n",
        "    self.sa = MultiheadAttention(n_head,head_size)\n",
        "    self.feedforward = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.feedforward(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range (n_layer)])\n",
        "    self.lnf = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None ):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.lnf(x)\n",
        "    x = self.feedforward(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # print(\"targets \\n\",targets)\n",
        "      B,T,C = logits.shape\n",
        "      # print(\"logits \\n\",logits.shape)\n",
        "      logits = logits.view(B*T,C)\n",
        "      # print(\"logits reshape \\n\", logits.shape)\n",
        "      targets = targets.view(B*T)\n",
        "      # print(\"targets reshape \\n\", targets.shape)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "      # print(\"loss \\n\",loss)\n",
        "\n",
        "    return logits , loss\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for k in range(max_new_tokens):\n",
        "      idx_cond = idx[:,-block_size:]\n",
        "      logits, loss = self(idx_cond)\n",
        "      # if k == 0 :\n",
        "      #   print(logits.shape)\n",
        "      logits= logits[:,-1, :]\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "cJmPqn638qR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}